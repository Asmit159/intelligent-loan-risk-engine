MODEL EVALUATION & BIAS–VARIANCE ANALYSIS


1. Objective of Model Evaluation

The goal of model evaluation is not just to measure accuracy, but to understand:

-How well the model generalizes to unseen applicants
-What types of mistakes it makes
-Whether the model is biased toward approval or rejection
-Whether the model is underfitting or overfitting

This is especially critical in loan approval systems, where errors have financial and ethical consequences.



2. Why Accuracy Alone Is Not Sufficient

-Loan approval datasets are often imbalanced (more approvals than rejections or vice versa).
-A naïve model that always predicts “Approved” could still achieve high accuracy while being financially dangerous.
-Therefore, multiple evaluation metrics are required.



3. Primary Evaluation Metrics Used:

3.1 Confusion Matrix

The confusion matrix breaks predictions into four categories:

True Positive (TP): Approved loan correctly approved
False Positive (FP): Risky loan incorrectly approved
True Negative (TN): Risky loan correctly rejected
False Negative (FN): Safe loan incorrectly rejected

In loan systems:

False Positives are more costly (defaults)
False Negatives impact customer experience.


3.2 Precision (Bank Safety Metric)

Precision answers:
“Of all loans we approved, how many were actually safe?”

High precision means:
-Fewer risky loans are approved
-Lower default risk for the bank

This metric is critical for financial risk control.


3.3 Recall (Customer-Friendliness Metric)

Recall answers:
“Of all truly safe applicants, how many did we approve?”

High recall means:
-Fewer deserving customers are rejected
-Better customer satisfaction
-Banks typically balance recall against precision.


3.4 ROC–AUC Score (Overall Model Quality)

ROC–AUC measures the model’s ability to separate approved vs rejected applicants across all thresholds.
Interpretation:

AUC ≈ 0.5 → random guessing

AUC ≈ 0.7–0.8 → good classical ML model

AUC > 0.8 → strong separation

This metric is threshold-independent and highly informative.





4. Decision Threshold Analysis


The logistic regression model outputs a probability, not a hard decision.

Instead of using a default threshold (0.5), thresholds are adjusted based on business goals:

Higher threshold → safer approvals, fewer defaults

Lower threshold → higher approval rate, higher risk

Threshold tuning allows the system to:

Adapt to market conditions

Balance growth vs safety

This reinforces the separation between ML prediction and business decision logic.




5. Bias–Variance Analysis

Understanding model errors helps improve reliability.


5.1 High Bias (Underfitting)

Symptoms:
Poor performance on both training and validation data
Approval probabilities clustered around the mean

Causes:
Oversimplified feature sets
Important financial signals not captured

Mitigation:
Add meaningful engineered features (e.g., asset ratios)
Improve feature representation



5.2 High Variance (Overfitting)

Symptoms:
Very good training performance
Poor validation/test performance

Causes:

Model too sensitive to noise
Rare patterns overly emphasized

Mitigation:
More training data
Regularization

Removing unstable or redundant features





6. Fairness and Stability Considerations

Although this project focuses on classical ML, care is taken to:

-Avoid hard-coded discriminatory rules
-Rely on financial capacity and asset-based features
-Use explainable coefficients for auditability
-This aligns with real-world expectations in regulated lending systems.





7. Model Interpretability

Logistic regression coefficients provide insight into:

-Which features increase approval probability
-Which features increase risk

This allows:

-Business validation of model behavior
-Transparent explanation to stakeholders
-Easier debugging and improvement
-Interpretability is a key reason logistic regression is preferred in this system.





8. Summary of Evaluation Strategy

-Multiple metrics are used instead of accuracy alone
-Precision and recall are aligned with bank and customer objectives
-ROC–AUC measures overall separation quality
-Bias–variance analysis guides future improvements
-Thresholds remain a business-controlled decision


This evaluation framework ensures the model is reliable, safe, and production-aligned.



END OF MODEL EVALUATION & BIAS–VARIANCE ANALYSIS.EOF